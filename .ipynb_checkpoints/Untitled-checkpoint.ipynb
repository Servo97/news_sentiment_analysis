{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os, re, csv, codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, LeakyReLU\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel('Data_Train.xlsx')\n",
    "test = pd.read_excel('Data_Test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                               STORY  SECTION\n",
       " 0  But the most painful was the huge reversal in ...        3\n",
       " 1  How formidable is the opposition alliance amon...        0\n",
       " 2  Most Asian currencies were trading lower today...        3\n",
       " 3  If you want to answer any question, click on ‘...        1\n",
       " 4  In global markets, gold prices edged up today ...        3,\n",
       "                                                STORY\n",
       " 0  2019 will see gadgets like gaming smartphones ...\n",
       " 1  It has also unleashed a wave of changes in the...\n",
       " 2  It can be confusing to pick the right smartpho...\n",
       " 3  The mobile application is integrated with a da...\n",
       " 4  We have rounded up some of the gadgets that sh...)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(),test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(STORY      False\n",
       " SECTION    False\n",
       " dtype: bool, STORY    False\n",
       " dtype: bool)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().any(),test.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    0\n",
       "2    3\n",
       "3    1\n",
       "4    3\n",
       "Name: SECTION, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train['SECTION']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros((y.size, y.max()+1))\n",
    "b[np.arange(y.size), y] = 1\n",
    "y=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    But the most painful was the huge reversal in ...\n",
       " 1    How formidable is the opposition alliance amon...\n",
       " 2    Most Asian currencies were trading lower today...\n",
       " 3    If you want to answer any question, click on ‘...\n",
       " 4    In global markets, gold prices edged up today ...\n",
       " Name: STORY, dtype: object,\n",
       " 0    2019 will see gadgets like gaming smartphones ...\n",
       " 1    It has also unleashed a wave of changes in the...\n",
       " 2    It can be confusing to pick the right smartpho...\n",
       " 3    The mobile application is integrated with a da...\n",
       " 4    We have rounded up some of the gadgets that sh...\n",
       " Name: STORY, dtype: object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train['STORY']\n",
    "x_test = test['STORY']\n",
    "x_train.head(),x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71229\n",
      "21577\n"
     ]
    }
   ],
   "source": [
    "a= set()\n",
    "freq = {}\n",
    "for i in x_train:\n",
    "    for j in i.split():\n",
    "        a.add(j)\n",
    "        if j in freq.keys():\n",
    "            freq[j]+=1\n",
    "        else:\n",
    "            freq[j]=1\n",
    "print(len(a))\n",
    "b=0\n",
    "for i in freq:\n",
    "    if freq[i]>=3:\n",
    "        b+=1\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 25000\n",
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(list(x_train))\n",
    "tokenized_train = tokenizer.texts_to_sequences(x_train)\n",
    "tokenized_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gigantic Output, Run at own risk.\n",
    "#print(tokenizer.word_counts,tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    But the most painful was the huge reversal in ...\n",
       " Name: STORY, dtype: object,\n",
       " [[30,\n",
       "   1,\n",
       "   94,\n",
       "   7406,\n",
       "   24,\n",
       "   1,\n",
       "   896,\n",
       "   8784,\n",
       "   5,\n",
       "   3234,\n",
       "   574,\n",
       "   21796,\n",
       "   4,\n",
       "   182,\n",
       "   658,\n",
       "   762,\n",
       "   2835,\n",
       "   2836,\n",
       "   12,\n",
       "   674,\n",
       "   10,\n",
       "   1347,\n",
       "   220,\n",
       "   556,\n",
       "   12,\n",
       "   9,\n",
       "   5157,\n",
       "   10,\n",
       "   2312,\n",
       "   8,\n",
       "   6894,\n",
       "   1213,\n",
       "   1948,\n",
       "   15,\n",
       "   19,\n",
       "   1348,\n",
       "   3,\n",
       "   3067,\n",
       "   9,\n",
       "   6479,\n",
       "   8,\n",
       "   23,\n",
       "   2837,\n",
       "   14,\n",
       "   5717,\n",
       "   1006,\n",
       "   21797,\n",
       "   1,\n",
       "   2312,\n",
       "   3235,\n",
       "   2,\n",
       "   95,\n",
       "   1213,\n",
       "   1948,\n",
       "   491,\n",
       "   248,\n",
       "   1,\n",
       "   13025,\n",
       "   5718,\n",
       "   13,\n",
       "   82,\n",
       "   9792,\n",
       "   2,\n",
       "   1415,\n",
       "   2,\n",
       "   6,\n",
       "   5719,\n",
       "   3441,\n",
       "   3236,\n",
       "   4,\n",
       "   21798,\n",
       "   3234,\n",
       "   574,\n",
       "   1315,\n",
       "   66,\n",
       "   3340,\n",
       "   95,\n",
       "   6479,\n",
       "   15864,\n",
       "   456,\n",
       "   2,\n",
       "   15865,\n",
       "   352,\n",
       "   1361,\n",
       "   674,\n",
       "   10,\n",
       "   68,\n",
       "   15,\n",
       "   19,\n",
       "   113,\n",
       "   13026,\n",
       "   8037,\n",
       "   5,\n",
       "   1,\n",
       "   437,\n",
       "   25,\n",
       "   7,\n",
       "   274,\n",
       "   347,\n",
       "   1866,\n",
       "   10,\n",
       "   280,\n",
       "   669,\n",
       "   6,\n",
       "   1635,\n",
       "   1172,\n",
       "   3,\n",
       "   13027,\n",
       "   4747,\n",
       "   30,\n",
       "   68,\n",
       "   7,\n",
       "   113,\n",
       "   1316,\n",
       "   204,\n",
       "   4214,\n",
       "   3,\n",
       "   1,\n",
       "   1526,\n",
       "   4,\n",
       "   6,\n",
       "   444,\n",
       "   3,\n",
       "   1658,\n",
       "   1482,\n",
       "   3341,\n",
       "   186,\n",
       "   11,\n",
       "   77,\n",
       "   6895,\n",
       "   14,\n",
       "   133,\n",
       "   280,\n",
       "   15,\n",
       "   22,\n",
       "   2,\n",
       "   452,\n",
       "   36,\n",
       "   1,\n",
       "   1547,\n",
       "   4,\n",
       "   8785,\n",
       "   153,\n",
       "   6,\n",
       "   1526,\n",
       "   129,\n",
       "   18,\n",
       "   1173]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:1],tokenized_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = [len(i) for i in tokenized_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(num_words,bins=np.arange(0,500,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Studying the above histogram we set max length of sentences to 300\n",
    "#Most sentences can be covered inside this length.\n",
    "max_len = 300\n",
    "ip_train = pad_sequences(tokenized_train,maxlen=max_len)\n",
    "ip_test = pad_sequences(tokenized_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,embed_size,input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(100,\n",
    "                             kernel_initializer='glorot_normal',recurrent_regularizer=l2(0.01),\n",
    "                             return_sequences=True,dropout=0.1,recurrent_dropout=0.1)))\n",
    "model.add(LSTM(100,\n",
    "               kernel_initializer='glorot_normal',recurrent_regularizer=l2(0.01),\n",
    "               dropout=0.1,recurrent_dropout=0.1))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(4,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 300, 256)          6400000   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 300, 200)          285600    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 6,819,444\n",
      "Trainable params: 6,819,444\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6865 samples, validate on 763 samples\n",
      "Epoch 1/10\n",
      "6865/6865 [==============================] - ETA: 4:24 - loss: 3.6933 - acc: 0.460 - ETA: 2:45 - loss: 3.6448 - acc: 0.589 - ETA: 2:12 - loss: 3.5970 - acc: 0.642 - ETA: 1:54 - loss: 3.5499 - acc: 0.669 - ETA: 1:44 - loss: 3.5031 - acc: 0.685 - ETA: 1:37 - loss: 3.4566 - acc: 0.696 - ETA: 1:31 - loss: 3.4109 - acc: 0.703 - ETA: 1:26 - loss: 3.3659 - acc: 0.709 - ETA: 1:23 - loss: 3.3212 - acc: 0.714 - ETA: 1:20 - loss: 3.2762 - acc: 0.717 - ETA: 1:17 - loss: 3.2311 - acc: 0.720 - ETA: 1:14 - loss: 3.1865 - acc: 0.723 - ETA: 1:11 - loss: 3.1421 - acc: 0.725 - ETA: 1:09 - loss: 3.0982 - acc: 0.726 - ETA: 1:06 - loss: 3.0559 - acc: 0.728 - ETA: 1:04 - loss: 3.0126 - acc: 0.729 - ETA: 1:01 - loss: 2.9720 - acc: 0.730 - ETA: 1:00 - loss: 2.9329 - acc: 0.732 - ETA: 58s - loss: 2.8957 - acc: 0.732 - ETA: 56s - loss: 2.8589 - acc: 0.73 - ETA: 54s - loss: 2.8222 - acc: 0.73 - ETA: 52s - loss: 2.7879 - acc: 0.73 - ETA: 50s - loss: 2.7536 - acc: 0.73 - ETA: 49s - loss: 2.7204 - acc: 0.73 - ETA: 47s - loss: 2.6879 - acc: 0.73 - ETA: 45s - loss: 2.6564 - acc: 0.73 - ETA: 43s - loss: 2.6263 - acc: 0.73 - ETA: 41s - loss: 2.5960 - acc: 0.73 - ETA: 39s - loss: 2.5666 - acc: 0.73 - ETA: 38s - loss: 2.5379 - acc: 0.73 - ETA: 36s - loss: 2.5096 - acc: 0.73 - ETA: 34s - loss: 2.4820 - acc: 0.73 - ETA: 32s - loss: 2.4548 - acc: 0.74 - ETA: 31s - loss: 2.4280 - acc: 0.74 - ETA: 29s - loss: 2.4015 - acc: 0.74 - ETA: 27s - loss: 2.3754 - acc: 0.74 - ETA: 26s - loss: 2.3504 - acc: 0.74 - ETA: 24s - loss: 2.3262 - acc: 0.74 - ETA: 23s - loss: 2.3027 - acc: 0.74 - ETA: 21s - loss: 2.2789 - acc: 0.74 - ETA: 19s - loss: 2.2557 - acc: 0.74 - ETA: 18s - loss: 2.2327 - acc: 0.74 - ETA: 16s - loss: 2.2103 - acc: 0.74 - ETA: 15s - loss: 2.1886 - acc: 0.74 - ETA: 13s - loss: 2.1669 - acc: 0.74 - ETA: 11s - loss: 2.1460 - acc: 0.74 - ETA: 10s - loss: 2.1251 - acc: 0.74 - ETA: 8s - loss: 2.1045 - acc: 0.7433 - ETA: 7s - loss: 2.0838 - acc: 0.743 - ETA: 5s - loss: 2.0638 - acc: 0.744 - ETA: 4s - loss: 2.0444 - acc: 0.744 - ETA: 2s - loss: 2.0251 - acc: 0.745 - ETA: 0s - loss: 2.0069 - acc: 0.745 - 86s 13ms/step - loss: 1.9948 - acc: 0.7461 - val_loss: 0.9821 - val_acc: 0.8070\n",
      "Epoch 2/10\n",
      "6865/6865 [==============================] - ETA: 1:16 - loss: 0.9700 - acc: 0.793 - ETA: 1:17 - loss: 0.9615 - acc: 0.794 - ETA: 1:14 - loss: 0.9431 - acc: 0.806 - ETA: 1:12 - loss: 0.9244 - acc: 0.821 - ETA: 1:11 - loss: 0.9051 - acc: 0.831 - ETA: 1:09 - loss: 0.8879 - acc: 0.838 - ETA: 1:07 - loss: 0.8713 - acc: 0.842 - ETA: 1:06 - loss: 0.8527 - acc: 0.849 - ETA: 1:05 - loss: 0.8389 - acc: 0.850 - ETA: 1:03 - loss: 0.8224 - acc: 0.852 - ETA: 1:02 - loss: 0.8063 - acc: 0.856 - ETA: 1:01 - loss: 0.7912 - acc: 0.859 - ETA: 59s - loss: 0.7790 - acc: 0.860 - ETA: 58s - loss: 0.7657 - acc: 0.86 - ETA: 57s - loss: 0.7517 - acc: 0.86 - ETA: 55s - loss: 0.7374 - acc: 0.86 - ETA: 54s - loss: 0.7250 - acc: 0.87 - ETA: 53s - loss: 0.7147 - acc: 0.87 - ETA: 51s - loss: 0.7030 - acc: 0.87 - ETA: 50s - loss: 0.6907 - acc: 0.87 - ETA: 48s - loss: 0.6818 - acc: 0.87 - ETA: 47s - loss: 0.6707 - acc: 0.88 - ETA: 45s - loss: 0.6607 - acc: 0.88 - ETA: 44s - loss: 0.6509 - acc: 0.88 - ETA: 42s - loss: 0.6419 - acc: 0.88 - ETA: 41s - loss: 0.6336 - acc: 0.88 - ETA: 39s - loss: 0.6266 - acc: 0.89 - ETA: 38s - loss: 0.6168 - acc: 0.89 - ETA: 36s - loss: 0.6073 - acc: 0.89 - ETA: 35s - loss: 0.5993 - acc: 0.89 - ETA: 33s - loss: 0.5913 - acc: 0.89 - ETA: 32s - loss: 0.5846 - acc: 0.90 - ETA: 30s - loss: 0.5772 - acc: 0.90 - ETA: 29s - loss: 0.5713 - acc: 0.90 - ETA: 27s - loss: 0.5631 - acc: 0.90 - ETA: 26s - loss: 0.5556 - acc: 0.90 - ETA: 24s - loss: 0.5485 - acc: 0.90 - ETA: 23s - loss: 0.5405 - acc: 0.91 - ETA: 21s - loss: 0.5345 - acc: 0.91 - ETA: 20s - loss: 0.5266 - acc: 0.91 - ETA: 18s - loss: 0.5199 - acc: 0.91 - ETA: 17s - loss: 0.5132 - acc: 0.91 - ETA: 15s - loss: 0.5083 - acc: 0.91 - ETA: 14s - loss: 0.5026 - acc: 0.91 - ETA: 12s - loss: 0.4975 - acc: 0.91 - ETA: 11s - loss: 0.4924 - acc: 0.91 - ETA: 9s - loss: 0.4866 - acc: 0.9210 - ETA: 8s - loss: 0.4811 - acc: 0.921 - ETA: 7s - loss: 0.4767 - acc: 0.922 - ETA: 5s - loss: 0.4719 - acc: 0.923 - ETA: 3s - loss: 0.4665 - acc: 0.924 - ETA: 2s - loss: 0.4614 - acc: 0.925 - ETA: 0s - loss: 0.4562 - acc: 0.926 - 84s 12ms/step - loss: 0.4534 - acc: 0.9272 - val_loss: 0.2154 - val_acc: 0.9695\n",
      "Epoch 3/10\n",
      "6865/6865 [==============================] - ETA: 1:19 - loss: 0.1543 - acc: 0.986 - ETA: 1:19 - loss: 0.1491 - acc: 0.990 - ETA: 1:18 - loss: 0.1504 - acc: 0.989 - ETA: 1:19 - loss: 0.1454 - acc: 0.990 - ETA: 1:16 - loss: 0.1493 - acc: 0.988 - ETA: 1:14 - loss: 0.1528 - acc: 0.986 - ETA: 1:12 - loss: 0.1537 - acc: 0.985 - ETA: 1:10 - loss: 0.1506 - acc: 0.986 - ETA: 1:08 - loss: 0.1497 - acc: 0.987 - ETA: 1:06 - loss: 0.1482 - acc: 0.987 - ETA: 1:04 - loss: 0.1529 - acc: 0.985 - ETA: 1:03 - loss: 0.1518 - acc: 0.985 - ETA: 1:01 - loss: 0.1524 - acc: 0.985 - ETA: 59s - loss: 0.1494 - acc: 0.986 - ETA: 58s - loss: 0.1495 - acc: 0.98 - ETA: 56s - loss: 0.1474 - acc: 0.98 - ETA: 54s - loss: 0.1459 - acc: 0.98 - ETA: 53s - loss: 0.1436 - acc: 0.98 - ETA: 51s - loss: 0.1408 - acc: 0.98 - ETA: 50s - loss: 0.1392 - acc: 0.98 - ETA: 48s - loss: 0.1372 - acc: 0.98 - ETA: 47s - loss: 0.1354 - acc: 0.98 - ETA: 45s - loss: 0.1332 - acc: 0.98 - ETA: 44s - loss: 0.1328 - acc: 0.98 - ETA: 42s - loss: 0.1311 - acc: 0.98 - ETA: 41s - loss: 0.1288 - acc: 0.98 - ETA: 39s - loss: 0.1267 - acc: 0.98 - ETA: 38s - loss: 0.1256 - acc: 0.98 - ETA: 36s - loss: 0.1241 - acc: 0.98 - ETA: 35s - loss: 0.1223 - acc: 0.98 - ETA: 33s - loss: 0.1212 - acc: 0.98 - ETA: 32s - loss: 0.1204 - acc: 0.98 - ETA: 30s - loss: 0.1199 - acc: 0.98 - ETA: 29s - loss: 0.1191 - acc: 0.98 - ETA: 27s - loss: 0.1177 - acc: 0.98 - ETA: 26s - loss: 0.1162 - acc: 0.98 - ETA: 24s - loss: 0.1163 - acc: 0.98 - ETA: 23s - loss: 0.1156 - acc: 0.98 - ETA: 21s - loss: 0.1150 - acc: 0.98 - ETA: 20s - loss: 0.1147 - acc: 0.98 - ETA: 18s - loss: 0.1137 - acc: 0.98 - ETA: 17s - loss: 0.1125 - acc: 0.98 - ETA: 15s - loss: 0.1112 - acc: 0.98 - ETA: 14s - loss: 0.1100 - acc: 0.98 - ETA: 12s - loss: 0.1095 - acc: 0.98 - ETA: 11s - loss: 0.1094 - acc: 0.98 - ETA: 9s - loss: 0.1089 - acc: 0.9888 - ETA: 8s - loss: 0.1083 - acc: 0.988 - ETA: 6s - loss: 0.1073 - acc: 0.988 - ETA: 5s - loss: 0.1061 - acc: 0.988 - ETA: 3s - loss: 0.1054 - acc: 0.988 - ETA: 2s - loss: 0.1052 - acc: 0.988 - ETA: 0s - loss: 0.1044 - acc: 0.988 - 82s 12ms/step - loss: 0.1038 - acc: 0.9887 - val_loss: 0.1320 - val_acc: 0.9659\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6865/6865 [==============================] - ETA: 1:16 - loss: 0.0714 - acc: 0.988 - ETA: 1:15 - loss: 0.0602 - acc: 0.991 - ETA: 1:14 - loss: 0.0557 - acc: 0.993 - ETA: 1:12 - loss: 0.0572 - acc: 0.991 - ETA: 1:11 - loss: 0.0559 - acc: 0.991 - ETA: 1:09 - loss: 0.0537 - acc: 0.992 - ETA: 1:09 - loss: 0.0525 - acc: 0.992 - ETA: 1:08 - loss: 0.0521 - acc: 0.993 - ETA: 1:07 - loss: 0.0516 - acc: 0.993 - ETA: 1:05 - loss: 0.0500 - acc: 0.993 - ETA: 1:03 - loss: 0.0490 - acc: 0.994 - ETA: 1:02 - loss: 0.0481 - acc: 0.994 - ETA: 1:00 - loss: 0.0498 - acc: 0.993 - ETA: 59s - loss: 0.0489 - acc: 0.993 - ETA: 57s - loss: 0.0479 - acc: 0.99 - ETA: 56s - loss: 0.0477 - acc: 0.99 - ETA: 54s - loss: 0.0470 - acc: 0.99 - ETA: 53s - loss: 0.0485 - acc: 0.99 - ETA: 51s - loss: 0.0485 - acc: 0.99 - ETA: 50s - loss: 0.0484 - acc: 0.99 - ETA: 48s - loss: 0.0485 - acc: 0.99 - ETA: 46s - loss: 0.0477 - acc: 0.99 - ETA: 45s - loss: 0.0470 - acc: 0.99 - ETA: 44s - loss: 0.0462 - acc: 0.99 - ETA: 42s - loss: 0.0457 - acc: 0.99 - ETA: 41s - loss: 0.0449 - acc: 0.99 - ETA: 39s - loss: 0.0442 - acc: 0.99 - ETA: 38s - loss: 0.0439 - acc: 0.99 - ETA: 36s - loss: 0.0432 - acc: 0.99 - ETA: 35s - loss: 0.0438 - acc: 0.99 - ETA: 33s - loss: 0.0433 - acc: 0.99 - ETA: 32s - loss: 0.0427 - acc: 0.99 - ETA: 31s - loss: 0.0424 - acc: 0.99 - ETA: 29s - loss: 0.0425 - acc: 0.99 - ETA: 28s - loss: 0.0422 - acc: 0.99 - ETA: 26s - loss: 0.0419 - acc: 0.99 - ETA: 25s - loss: 0.0417 - acc: 0.99 - ETA: 23s - loss: 0.0413 - acc: 0.99 - ETA: 22s - loss: 0.0410 - acc: 0.99 - ETA: 20s - loss: 0.0408 - acc: 0.99 - ETA: 19s - loss: 0.0405 - acc: 0.99 - ETA: 17s - loss: 0.0402 - acc: 0.99 - ETA: 16s - loss: 0.0401 - acc: 0.99 - ETA: 14s - loss: 0.0397 - acc: 0.99 - ETA: 13s - loss: 0.0395 - acc: 0.99 - ETA: 11s - loss: 0.0394 - acc: 0.99 - ETA: 9s - loss: 0.0391 - acc: 0.9954 - ETA: 8s - loss: 0.0390 - acc: 0.995 - ETA: 6s - loss: 0.0388 - acc: 0.995 - ETA: 5s - loss: 0.0387 - acc: 0.995 - ETA: 3s - loss: 0.0384 - acc: 0.995 - ETA: 2s - loss: 0.0383 - acc: 0.995 - ETA: 0s - loss: 0.0379 - acc: 0.995 - 84s 12ms/step - loss: 0.0377 - acc: 0.9953 - val_loss: 0.1126 - val_acc: 0.9731\n",
      "Epoch 5/10\n",
      "6865/6865 [==============================] - ETA: 1:20 - loss: 0.0250 - acc: 0.996 - ETA: 1:16 - loss: 0.0211 - acc: 0.998 - ETA: 1:14 - loss: 0.0199 - acc: 0.998 - ETA: 1:12 - loss: 0.0186 - acc: 0.999 - ETA: 1:11 - loss: 0.0190 - acc: 0.998 - ETA: 1:10 - loss: 0.0215 - acc: 0.997 - ETA: 1:10 - loss: 0.0209 - acc: 0.997 - ETA: 1:08 - loss: 0.0201 - acc: 0.997 - ETA: 1:07 - loss: 0.0197 - acc: 0.998 - ETA: 1:05 - loss: 0.0191 - acc: 0.998 - ETA: 1:04 - loss: 0.0195 - acc: 0.997 - ETA: 1:03 - loss: 0.0201 - acc: 0.997 - ETA: 1:01 - loss: 0.0199 - acc: 0.997 - ETA: 1:00 - loss: 0.0199 - acc: 0.997 - ETA: 58s - loss: 0.0194 - acc: 0.997 - ETA: 57s - loss: 0.0205 - acc: 0.99 - ETA: 55s - loss: 0.0200 - acc: 0.99 - ETA: 54s - loss: 0.0196 - acc: 0.99 - ETA: 52s - loss: 0.0192 - acc: 0.99 - ETA: 51s - loss: 0.0187 - acc: 0.99 - ETA: 49s - loss: 0.0185 - acc: 0.99 - ETA: 47s - loss: 0.0187 - acc: 0.99 - ETA: 46s - loss: 0.0186 - acc: 0.99 - ETA: 45s - loss: 0.0183 - acc: 0.99 - ETA: 43s - loss: 0.0181 - acc: 0.99 - ETA: 42s - loss: 0.0179 - acc: 0.99 - ETA: 40s - loss: 0.0176 - acc: 0.99 - ETA: 38s - loss: 0.0180 - acc: 0.99 - ETA: 37s - loss: 0.0177 - acc: 0.99 - ETA: 35s - loss: 0.0180 - acc: 0.99 - ETA: 34s - loss: 0.0177 - acc: 0.99 - ETA: 32s - loss: 0.0177 - acc: 0.99 - ETA: 31s - loss: 0.0175 - acc: 0.99 - ETA: 29s - loss: 0.0179 - acc: 0.99 - ETA: 28s - loss: 0.0178 - acc: 0.99 - ETA: 26s - loss: 0.0182 - acc: 0.99 - ETA: 25s - loss: 0.0179 - acc: 0.99 - ETA: 23s - loss: 0.0178 - acc: 0.99 - ETA: 22s - loss: 0.0179 - acc: 0.99 - ETA: 20s - loss: 0.0177 - acc: 0.99 - ETA: 19s - loss: 0.0176 - acc: 0.99 - ETA: 17s - loss: 0.0174 - acc: 0.99 - ETA: 16s - loss: 0.0174 - acc: 0.99 - ETA: 14s - loss: 0.0171 - acc: 0.99 - ETA: 12s - loss: 0.0169 - acc: 0.99 - ETA: 11s - loss: 0.0169 - acc: 0.99 - ETA: 9s - loss: 0.0170 - acc: 0.9973 - ETA: 8s - loss: 0.0168 - acc: 0.997 - ETA: 6s - loss: 0.0168 - acc: 0.997 - ETA: 5s - loss: 0.0167 - acc: 0.997 - ETA: 3s - loss: 0.0166 - acc: 0.997 - ETA: 2s - loss: 0.0165 - acc: 0.997 - ETA: 0s - loss: 0.0164 - acc: 0.997 - 84s 12ms/step - loss: 0.0163 - acc: 0.9974 - val_loss: 0.0972 - val_acc: 0.9738\n",
      "Epoch 6/10\n",
      "6865/6865 [==============================] - ETA: 1:29 - loss: 0.0072 - acc: 1.000 - ETA: 1:23 - loss: 0.0118 - acc: 0.998 - ETA: 1:20 - loss: 0.0136 - acc: 0.997 - ETA: 1:18 - loss: 0.0138 - acc: 0.997 - ETA: 1:15 - loss: 0.0127 - acc: 0.997 - ETA: 1:13 - loss: 0.0133 - acc: 0.997 - ETA: 1:10 - loss: 0.0123 - acc: 0.997 - ETA: 1:08 - loss: 0.0128 - acc: 0.997 - ETA: 1:07 - loss: 0.0121 - acc: 0.997 - ETA: 1:05 - loss: 0.0120 - acc: 0.997 - ETA: 1:04 - loss: 0.0115 - acc: 0.997 - ETA: 1:02 - loss: 0.0123 - acc: 0.997 - ETA: 1:00 - loss: 0.0121 - acc: 0.997 - ETA: 59s - loss: 0.0118 - acc: 0.997 - ETA: 57s - loss: 0.0115 - acc: 0.99 - ETA: 55s - loss: 0.0111 - acc: 0.99 - ETA: 54s - loss: 0.0114 - acc: 0.99 - ETA: 53s - loss: 0.0111 - acc: 0.99 - ETA: 51s - loss: 0.0108 - acc: 0.99 - ETA: 49s - loss: 0.0107 - acc: 0.99 - ETA: 48s - loss: 0.0105 - acc: 0.99 - ETA: 46s - loss: 0.0102 - acc: 0.99 - ETA: 45s - loss: 0.0104 - acc: 0.99 - ETA: 43s - loss: 0.0101 - acc: 0.99 - ETA: 42s - loss: 0.0100 - acc: 0.99 - ETA: 40s - loss: 0.0099 - acc: 0.99 - ETA: 39s - loss: 0.0097 - acc: 0.99 - ETA: 37s - loss: 0.0097 - acc: 0.99 - ETA: 36s - loss: 0.0096 - acc: 0.99 - ETA: 34s - loss: 0.0094 - acc: 0.99 - ETA: 33s - loss: 0.0093 - acc: 0.99 - ETA: 31s - loss: 0.0091 - acc: 0.99 - ETA: 30s - loss: 0.0090 - acc: 0.99 - ETA: 29s - loss: 0.0088 - acc: 0.99 - ETA: 27s - loss: 0.0088 - acc: 0.99 - ETA: 26s - loss: 0.0091 - acc: 0.99 - ETA: 24s - loss: 0.0091 - acc: 0.99 - ETA: 23s - loss: 0.0092 - acc: 0.99 - ETA: 21s - loss: 0.0093 - acc: 0.99 - ETA: 20s - loss: 0.0092 - acc: 0.99 - ETA: 18s - loss: 0.0091 - acc: 0.99 - ETA: 17s - loss: 0.0090 - acc: 0.99 - ETA: 15s - loss: 0.0089 - acc: 0.99 - ETA: 14s - loss: 0.0088 - acc: 0.99 - ETA: 12s - loss: 0.0090 - acc: 0.99 - ETA: 11s - loss: 0.0090 - acc: 0.99 - ETA: 9s - loss: 0.0093 - acc: 0.9983 - ETA: 8s - loss: 0.0092 - acc: 0.998 - ETA: 6s - loss: 0.0093 - acc: 0.998 - ETA: 5s - loss: 0.0092 - acc: 0.998 - ETA: 3s - loss: 0.0091 - acc: 0.998 - ETA: 2s - loss: 0.0092 - acc: 0.998 - ETA: 0s - loss: 0.0092 - acc: 0.998 - 83s 12ms/step - loss: 0.0091 - acc: 0.9983 - val_loss: 0.1028 - val_acc: 0.9738\n",
      "Epoch 7/10\n",
      " 256/6865 [>.............................] - ETA: 1:18 - loss: 0.0040 - acc: 1.000 - ETA: 1:17 - loss: 0.0041 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-f2c23a2d83fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mip_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dlenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dlenv\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dlenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dlenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 6\n",
    "history = model.fit(ip_train,y,batch_size=batch_size,epochs=10,validation_split=0.1,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
